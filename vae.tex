\documentclass{article}
\usepackage{preamble}
\input{preamble}
\graphicspath{ {images/} }
%%%%%%%%%%%%%%%% Document %%%%%%%%%%%%%%%%
\begin{document}
\begin{center}
{\huge \underline{Variational Auto Encoders}}
\end{center}
\section{Introduction}
\begin{definition}[Statistical Inference] Determining properties of the underlying probability distribution based on observed data. This could include estimating parameters, latent variables, or making predictions. In Bayesian inference, the primary goal is to compute the posterior $p(z|x)$:

\[
  p(z|x) = \frac{p(x,z)}{p(x)} = \frac{p(x|z) p(z)}{\int_{\mathcal{Z}} p(x,z) \d{z} }
\]
  Since the marginal likelihood $p(x)$ is intractable, we instead approximate the posterior $p(z|x)$ with a simpler, tractable distribution $q(z| \lambda)$, and we wish to minimize the KL divergence $KL \Big( q(z | \lambda) \, || \, p(z | x) \Big)$, or equivalently maximize the Evidence Lower Bound (ELBO):


\end{definition}
\begin{definition}[Density Estimation] Approximating PDF $p(x)$ of a random variable $X$ based on observed data $\left\{ x_1, \ldots, x_n \right\}$ drawn i.i.d from the underlying distribution. Can be either parametric, e.g.
\[
  p(x | \theta) \sim  \mathcal{N}( \mu_x, \sigma_x)
\]
or nonparametric density estimation, for example Kernel Density Estimation:
\[
  p(x) = \frac{1}{n h^{d}} \sum_{i=1}^{n} K \left( \frac{x-x_i}{h} \right) 
\]
\end{definition}
\begin{definition}[Modeling] Modeling is unveiling the underlying ruling processes, by posing hypotheses and predictions, based on observations. For instance, physicists model how fluids flow, and biologists the  structure of organisms. 

Modeling often involves representation, where we describe a phenomena using specific qualities and quantities related to the process we are interested in. We describe an object by its shape, color, position, volume, etc. 
When looking at data - a large collection of samples - in some cases, it is reasonable to believe that those representations follow some distributions. For example, human height clearly follow some probability distribution. In such cases, we can think of the samples as being "generated" from those distributions. When such a hypothesis is true, we can generate new samples in the population, provided we estimated its probabilities. Alternatively, we may say that we have uncertainty about those variables, and we specify the degree and nature of this uncertainty  in terms of probability distributions.  
\end{definition}

\begin{remark} A complete probabilistic model captures both the \ul{distributions} of its components and the \ul{relations/dependencies} between them. Usually the linear relations, given by the covariances.
\end{remark}

\begin{definition}[Probabilistic Model] Assume the observed variables $\bm{x}$ are a random sample from an unknown underlying process, whose true probability distribution is $p^{\ast}(\bm{x})$. We approximate this underlying process with a chosen probabilistic model $p_{\theta}(\bm{x})$, with parameters $\bm{\theta}$:
\[
  \bm{x} \sim p_{\theta}(\bm{x}) 
\]
  Learning is the process of searching for a value of the parameters $\theta$, such that the probability function given by the mode $p_{\theta}(\bm{x})$, approximates the true distribution of the data, denoted by $p^{^{\ast}}(\bm{x})$, such that for any observed $\bm{x}$:
\[
  p_{\theta}(\bm{x}) \approx p ^{\star}(\bm{x}) 
\]
\end{definition}

\begin{definition}[Conditional Models] Often, we are not interested in learning an unconditional model $p_{\theta}(\bm{x})$, but a conditional model $p_{\theta}(\bm{y} | \bm{x})$, that approximates the underlying conditional distribution $p ^{\ast}(\bm{y} | \bm{x})$: A distribution over the values of variable $\bm{y}$, conditioned on the value of an observed variable $\bm{x}$. $\bm{x}$ is often called the \textit{input} of the model. 

A common example is image classification, where $\bm{x}$ is the image, and $\bm{y}$ is the label, and $p_{\theta}(\bm{y}|\bm{x})$ is chosen to be the categorical distribution, whose parameters are computed from $\bm{x}$. 
\end{definition}

\begin{remark} We can use neural networks to parameterize a distribution. For example for the categorical distribution $\mathrm{Categorical}(y ; \bm{p})$ over a class label $y$, we have: 
\begin{gather*}
  \bm{p} = \mathrm{NeuralNet}(\bm{x}) \\
  p_{\theta}(y | \bm{x}) = \mathrm{Categorical}(y ; \bm{p})
\end{gather*}
\end{remark}

\subsection{Directed PGMs and NNs}
We work with \textit{directed} PGMs, where all variables are organized into a directed acyclic graph, and the joint distribution over the variables factorizes as a product of prior and conditional probabilities:
\[
  p_{\theta}(\bm{x}_1, \ldots  \bm{x}_m) = \sum_{j=1}^{m} p_{\theta}(\bm{x}_j | Pa(\bm{x}_j))    
\]
where $Pa(\bm{x}_j)$ is the set of parent variables of node $j$ in the directed graph. 

Traditionally, each conditional probability distribution $p_{\theta}(\bm{x}_j | Pa(\bm{x}_j))$ is parameterized by a lookup table or a linear model. A more flexible way is to parametrize such conditional distributions using a neural network. In this case, the NN takes as input the parents of a variable, and produce the distributional parameters $\bm{\eta}$ over that variable:
\begin{gather*}
  \bm{\eta} = \mathrm{NeuralNet}(Pa(\bm{x})) \\
  p_{\theta}(\bm{x} | Pa(\bm{x})) = p_{\theta}(\bm{x} | \bm{\eta}) 
\end{gather*}
for example, in VAE, when learning $p_{\theta}(\bm{x} | \bm{z})$, note that  $Pa(\bm{x}) = \bm{z}$, and the recognition model learn the distributional parameters $\bm{\eta}  = (\mu, \sigma)$  to parametrize $p_{\theta}(\bm{x} | \bm{\eta})$
\subsection{Learning in Fully Observed Models with Neural Nets}

If all variables in the directed model are observed in the data, then we can compute and differentiate the log-probability of the data under the model, leading to a relatively straightforward optimization. 

\ul{Dataset}: We collect a dataset $\mathcal{D}$ consisting of $N$ datapoints:
\[
  \mathcal{D} = \left\{ \bm{x}^{(1)}, \ldots, \bm{x}^{(N)} \right\} 
\]
The datapoints are assumed to be independent samples from an unchanging underlying distribution, i.e. i.i.d. Then the probability of the datapoints given the parameters factorizes as a product of individual datapoint probabilities:
\[ \begin{array}{@{} R{0.3\textwidth}  C{0.6\textwidth} P{0.1\textwidth} @{}} 
& \[
 \log p_{\theta}(\mathcal{D})  = \displaystyle \sum_{\bm{x} \in \mathcal{D}} \log  p_{\theta}(\bm{x}) \label{eq:log_likelihood} \tag{Maximum Likelihood}
\]  &   
\end{array} \]

A common criterion for probabilistic models is maximum log-likelihood (MLL). We attempt to find the parameters $\theta$ that maximize the above sum, or equivalently, the average, of the log-probabilities assigned to the data by the model. Since our model is limited by its expressiveness, and our data is noisy, this can not be trivially solved.  

Using stochastic gradient descent, we draw a minibatch $\mathcal{M}$ of the data, and since:
\[
  \nabla_{\theta} \log  p_{\theta}(\mathcal{D})  \simeq \nabla_{\theta} \log  p_{\theta} (\mathcal{M})  = \sum_{\bm{x} \in \mathcal{M}} \nabla_{\theta}  \log  p_{\theta}(\bm{x})
\]
where $\simeq$ indicates an unbiased estimator, we can use $\displaystyle \sum_{\bm{x} \in \mathcal{M}} \nabla_{\theta} \log  p_{\theta}(\bm{x})$ to iteratively update $\theta$ and  hill-climb to a local optimum of the~\ref{eq:log_likelihood}.  

\subsection{Learning and Inference in Deep Latent Variable Models}
\begin{definition}[Latent Variables]  Variables which are part of the model, but which we do not observe.

For an observed variable $\bm{x}$ and an unobserved variable $\bm{z}$, the joint distribution is denoted $p_{\theta}(\bm{x}, \bm{z})$ over both variables. The marginal distribution over the observed variables $p_{\theta}(\bm{x})$ is given by:
  \begin{equation} \label{eq:marginalLikelihood}
  p_{\theta}(\bm{x}) = \int p_{\theta}(\bm{x}, \bm{z}) \d{\bm{z}}
\end{equation}
This is also called the \textit{evidence}.
\end{definition}

\begin{definition}[Deep Latent Variable Model (DLVM)]  latent variable model $p_{\theta}(\bm{x}, \bm{z})$ whose distributions are parameterized by neural nets. Then, even when each factor (prior or conditional distribution) in the directed model is relatively simple (such as a conditional Gaussian), the marginal distribution $p_{\theta}(\bm{x})$ can be very complex. This expressivity makes DLVM attractive for approximating complicated underlying distributions $p ^{\star}(\bm{x})$. 
\end{definition}

\begin{example}  A simple DLVM is:
\[
  p_{\theta}(\bm{x}, \bm{z}) = p_{\theta}(\bm{z}) p_{\theta}(\bm{x} | \bm{z}) 
\]
we call $p_{\theta}(\bm{z})$ the \textit{prior distribution} over $\bm{z}$. 

  For example, assume, for binary data $\bm{x}$ (such as 0-1 MNIST images), a spherical Gaussian latent space, and a factorized Bernoulli observation model. Then:
\begin{gather*}
  p (\bm{z}) = \mathcal{N}( \bm{z} ; 0, I) \\
  \bm{p} = \mathrm{DecoderNeuralNet}_{\theta} (\bm{z}) \\
  \log p( \bm{x} | \bm{z}) = \sum_{j=1}^{D} \log  p (x_j | \bm{z}) = \sum_{j=1}^{D} \log \mathrm{Bernoulli}( x_{j} ; p_{j} )
\end{gather*}
where $D$ is the dimensionality of $\bm{x}$. 
\end{example}

\begin{remark} As $\bm{z}$ is not observed, computing~\cref{eq:marginalLikelihood} is intractable, and hence we cannot directly optimize it. 

  For a Fully observed model, like $p_{\theta}(\bm{x}) \sim \mathcal{N}( \mu, \sigma ^2)$, we can easily compute $\log p_{\theta}(\bm{x})$, and optimize $\theta$ using the gradients of~\ref{eq:log_likelihood}. 

For latent variable models, we have:
\[
  p_{\theta}(\bm{x}) = \int p_{\theta}(\bm{x}, \bm{z}) \d{\bm{z}}
\]
The integral over $\bm{z}$ often lacks a closed-form solution, due to the complexity of taking its gradient,  and the dimensionality of $\bm{z}$. 

  Note - we often do know $p_{\theta}(\bm{x}, \bm{z})$, the joint distribution, since we have $p_{\theta}(\bm{x}, \bm{z}) =  p_{\theta}(\bm{x} | z) p_{\theta}(\bm{z})$, and we often assume $p_{\theta}(\bm{z})$, and $p_{\theta}(\bm{x} | z)$, the \textit{likelihood} 

The intractability of $p_{\theta}(\bm{x})$ is related to the intractability of the \textit{posterior} distribution $p_{\theta}(\bm{z} | \bm{x})$, via the identity:
\[
  p_{\theta}(\bm{z} | \bm{x}) = \frac{p_{\theta}(\bm{x}, \bm{z})}{ p_{\theta}(\bm{x})}
\]
  since $p_{\theta}(\bm{x}, \bm{z})$ is tractable to compute. We approximate $p_{\theta}(\bm{z} | \bm{x})$ using variational inference, by a similar distribution $q \theta(\bm{z} | \bm{x})$, called the \textbf{variational distribution}. 
\end{remark}

\section{Variational Auto Encoders}
\begin{remark}[Motivation] Suppose we have observed data \(\mathbf{x}\), for instance, pixels of dog images. These data points are drawn from some true but unknown distribution \(p^{\ast}(\mathbf{x})\). Our goal is to approximate $p ^{\ast}(\bm{x})$ using a parameterized family \(p_{\theta}(\mathbf{x})\), enabling tasks like generation (sampling new images) or downstream inference (classification, etc.).

Directly modeling \(p_{\theta}(\mathbf{x})\) can be extremely challenging, especially for complex, high-dimensional data. One common approach is to introduce latent variables \(\mathbf{z}\) that describe underlying factors or structures, and then express the data distribution through marginalization:
\[
p_{\theta}(\mathbf{x}) = \int p_{\theta}(\mathbf{x} \mid \mathbf{z}) p_{\theta}(\mathbf{z})\,d\mathbf{z}.
\]

Here, \(p_{\theta}(\mathbf{x}\mid\mathbf{z})\) is often simpler to model than \(p_{\theta}(\mathbf{x})\) itself. For example, \(\mathbf{z}\) might represent attributes like a dog's breed, pose, or lighting, making it easier to build a model that generates images \(\mathbf{x}\) given \(\mathbf{z}\). We also choose a prior \(p_{\theta}(\mathbf{z})\) that is tractable, such as a Gaussian.

The challenge is that computing \(p_{\theta}(\mathbf{x})\) requires integrating over \(\mathbf{z}\), which is typically intractable for complex models. This makes it hard to directly optimize parameters \(\theta\) by maximum likelihood.

\textbf{Role of the Posterior and Variational Inference}:
  Remember our goal is to find parameters $\theta$ that will optimize $\log p_{\theta}(\bm{x})$. Using mathematical tricks, we use posterior $p_{\theta}(\bm{z} | \bm{x})$ - in fact an approximation $q_{\phi}(\bm{z} | \bm{x})$ - and by choosing $\phi$ such that $q_{\phi}( \bm{z} | \bm{z})$ is close to $p_{\theta}(\bm{z})$, we increase $\log p_{\theta}(\bm{x})$. 

Specifically, note that:
\[
  \log p_{\theta}(\bm{x}) = \log \int p_{\theta}(\bm{x} | \bm{z}) p_{\theta}(\bm{z}) \d{\bm{z}}
\]

We Multiply and divide by  $q_{\phi}(\bm{z} | \bm{x})$:
\[
  \log p_{\theta}(\bm{x}) = \log \int q_{\phi}(\bm{z} | \bm{x}) \frac{ p_{\theta}(\bm{x}, \bm{z})}{q_{\phi}(\bm{z} | \bm{x})} \d{\bm{z}}
\]
Recall Jensen inequality for concave function states that $f(E[X]) \geq E[f(X)]$, and since $\log$ is concave, we get that $\log E[X] \geq E [\log X]$. Define $X = \frac{p_{\theta}(\bm{x}, \bm{z})}{q_{\phi}(\bm{z} | \bm{x})}$, and we get:
\[
  \log p_{\theta}(\bm{x}) \geq \int q_{\phi}(\bm{z} | \bm{x}) \log \frac{p_{\theta}(\bm{x}, \bm{z})}{q_{\phi}(\bm{z} | \bm{x})} \d{\bm{z}}
\]

Rewrite the integrand using $p_{\theta}(\bm{x}, \bm{z}) = p_{\theta}(\bm{x} | \bm{z}) p_{\theta}(\bm{z})$ and we get:
\[
\log p_{\theta}(\bm{x}) \geq \int q_{\phi}(\bm{z} | \bm{x}) \log \frac{p_{\theta}(\bm{x} | \bm{z}) p_{\theta}(\bm{z})}{q_{\phi}(\bm{z} | \bm{x})}  \d{\bm{z}}
\]
This gives us:
\begin{align*}
  \log p_{\theta}(\bm{x}) & \geq E_{q_{\phi}(\bm{z} | \bm{x})}\big[  p_{\theta}(\bm{x} | \bm{z}) \big]   + E_{q_{\phi}(\bm{z} | \bm{x})} \big[  \log p_{\theta}(\bm{z}) \big] - E_{q_{\phi}(\bm{z} | \bm{x})} \big[ \log q_{\phi}(\bm{z} | \bm{x}) \big] \\
  &= E_{q_{\phi}(\bm{z} | \bm{x})}\big[  p_{\theta}(\bm{x} | \bm{z}) \big]   -  \Big[ E_{q_{\phi}(\bm{z} | \bm{x})} \big[ \log q_{\phi}(\bm{z} | \bm{x}) \big]  - E_{q_{\phi}(\bm{z} | \bm{x})} \big[  \log p_{\theta}(\bm{z}) \big]   \Big]  \\
  &= E_{q_{\phi}(\bm{z} | \bm{x})}\big[  p_{\theta}(\bm{x} | \bm{z}) \big]  - KL \Big( q_{\phi}(\bm{z} | \bm{x})  || p_{\theta}(\bm{z}) \Big) 
\end{align*}

This is the Evidence Lower BOund (ELBO):
\[
\log p_{\theta}(\mathbf{x}) \ge \underbrace{\mathbb{E}_{q_{\phi}(\mathbf{z}\mid\mathbf{x})}[\log p_{\theta}(\mathbf{x}\mid\mathbf{z})] - \text{KL}(q_{\phi}(\mathbf{z}\mid\mathbf{x})\|p_{\theta}(\mathbf{z}))}_{\text{ELBO}(\theta,\phi)}.
\]

If we fix $\theta$ and adjust $\phi$, making $q_{\phi}(\bm{z} | \bm{x})$ closer to $p_{\theta}(\bm{z})$ we increase $\log p_{\theta}(\bm{x})$.


In summary, latent variable models and the introduction of a variational posterior allow us to deal with intractable marginal likelihoods by converting the problem into one of optimizing a tractable lower bound, thereby enabling effective parameter learning even for very complex data like images.
\end{remark}




\begin{definition}[VAE] The VAE can be viewed  as two coupled, but independently parameterized models: The encoder or the recognition model,and the generation model (aka decoder). The recognition model provides (an approximation to) the posterior for latent variables conditioned on observed data $p_{\theta}(z|x)$. 
\end{definition}

\end{document}

